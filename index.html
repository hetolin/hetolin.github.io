<!DOCTYPE HTML>
<html>
    <head>
        <title>Haitao Lin's Homepage</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1000">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="style.css" />

        <script>
             (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
             m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
             ga('create', 'UA-89797207-1', 'auto');
             ga('send', 'pageview');
       </script>
    </head>
    <body id="body">
        <div id="main">
            <header id="header">
                <a href="index.html">HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
<!--                <a href="https://real.stanford.edu/">LAB</a>-->
            </header>
            <div id="profile">
<!--                <div id="profile-pic">-->
<!--                    <img src="resources/images/people/haitao_lin.jpg">-->
<!--                    <p>-->
<!--                        <a href="https://scholar.google.com.hk/citations?user=ibTwXXsAAAAJ&hl=zh-CN">Google Scholar</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://space.bilibili.com/472147012">Bilibili</a> &nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://www.youtube.com/channel/UCcJThOdmzagbiOevT6IDbpQ">YouTube</a>-->
<!--                    </p>-->
<!--                </div>-->

                <div id="profile-pic" >
                    <img src="resources/images/people/hetolin.JPG">
                    <p>
                        <a href="https://scholar.google.com.hk/citations?user=ibTwXXsAAAAJ&hl=zh-CN">Google Scholar</a>ï½œ<a href="https://space.bilibili.com/472147012">Bilibili</a>ï½œ<a href="https://www.youtube.com/channel/UCcJThOdmzagbiOevT6IDbpQ">YouTube</a>
                        <br>
                        Email: hetolin[at]tencent[dot]com

                    </p>
                </div>

                <div id="profile-intro">
                    <div id="profile-name">æž—æµ·æ¶›ï½œHaitao Lin @ Tencent Robotics X ðŸ¦¾</div>
<!--                    <div id="profile-name" style="font-size: larger;">æž—æµ·æ¶›ï½œHaitao Lin</div>-->

                    <p style="text-align:justify; text-justify:inter-ideograph;">
                        I am a senior researcher at <a href="https://roboticsx.tencent.com/#/">Tencent Robotics X Lab</a>.  I obtained my Ph.D. degree in Computer Science (Outstanding Graduates of Shanghai) from Fudan University in 2024, advised by Prof.<a href="https://faculty.fudan.edu.cn/xyxue/zh_CN/index.htm">Xiangyang Xue</a> and Prof.<a href="http://yanweifu.github.io">Yanwei Fu</a>.
<!--                        During my PhD journey, I had the opportunity to spend wonderful summers as a research intern at <a href="https://www.mech-mind.com.cn">Mech-Mind Robotics</a>. Prior to this, I received my bachelorâ€™s degree in Electrical Engineering from Fuzhou University in June 2019.-->
<!--                        I obtained my Ph.D. degree from <a href="https://faet.fudan.edu.cn">Academic for Engineering & Technology</a>, Fudan University, Shanghai, China.-->
<!--                        I am advised by Prof.<a href="https://faculty.fudan.edu.cn/xyxue/zh_CN/index.htm">Xiangyang Xue</a> and co-advised by Prof.<a href="http://yanweifu.github.io">Yanwei Fu</a>.-->
<!--                        During my PhD journey, I had the opportunity to spend wonderful summers as a research intern at <a href="https://www.mech-mind.com.cn">Mech-Mind Robotics</a>. Prior to this, I received my bachelorâ€™s degree in Electrical Engineering from Fuzhou University in June 2019.-->
                        <br />
                        <br />
                        My research interest lies at Robotics and 3D Computer Vision. Specifically, I am very interested in the foundation model for general robotic manipulation.
                        <br />
                    </p>
<!--                    <p>-->
<!--                        Email: hetolin@tencent.com <br>-->
<!--                    </p>-->
                    <br>
                    <p>
                        <span class="highlight">Actively looking for self-motivated interns to work on related research topics. Please feel free to drop me an email.</span>
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>

<!--            <div class="section recent-work">-->
<!--                <div class="slider">-->

<!--                  <a href="http://diffusion-policy.cs.columbia.edu/"><img src="images/projects/diffusion.gif"></a>-->
<!--                  <a href="http://dextairity.cs.columbia.edu/"><img src="images/projects/air.gif"></a>-->
<!--                    <a href="http://irp.cs.columbia.edu/"><img src="images/projects/irp.jpeg"></a>-->
<!--                    <a href="http://flingbot.cs.columbia.edu/"><img src="https://flingbot.cs.columbia.edu/images/teaser.png"></a>-->
<!--                    <a href="http://dsr-net.cs.columbia.edu/"><img src="images/projects/dsr.gif"></a>-->
<!--                    &lt;!&ndash; <a href="http://graspinwild.cs.columbia.edu/"><img src="images/projects/graspinwild.jpg"></a> &ndash;&gt;-->
<!--                    &lt;!&ndash; <a href="http://form2fit.github.io/"><img src="images/projects/form2fit.png"></a> &ndash;&gt;-->
<!--                    <a href="https://tossingbot.cs.princeton.edu/"><img src="images/projects/tossingbot.png"></a>-->
<!--                    &lt;!&ndash; <a href="https://sites.google.com/view/cleargrasp"><img src="images/projects/cleargrasp.gif"></a> &ndash;&gt;-->
<!--                    &lt;!&ndash;  <a href="https://spatial-action-maps.cs.princeton.edu/"><img src="images/projects/sam.png"></a> &ndash;&gt;-->
<!--                </div>-->
<!--            </div>-->

<!--            <div class="divider"></div>-->
<!--            <div class="section">-->
<!--                <h1>News</h1>-->
<!--                <p>-->
<!--                    <ul>-->
<!--                      <li> Gabilan Faculty Fellow, Stanford, 2023-->
<!--                      <li><a href="https://irp.cs.columbia.edu/">IRP</a> won <a href="https://roboticsconference.org/program/awards/"> Best Paper Award </a> at RSS 2022!-->
<!--                        <li><a href="https://dextairity.cs.columbia.edu/">DextAIRity</a> get selected as <a href="https://roboticsconference.org/program/awards/"> Best System Paper Finalist </a> at RSS 2022!-->


<!--                        <li> Received a <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2143601&HistoricalAwards=false"> NSF CAREER award </a> </li>-->

<!--                        <li> Honored to be a <a href="https://sloan.org/fellowships/2022-Fellows"> 2022 Sloan Research Fellow </a> </li>-->
<!--                        <li><a href="https://flingbot.cs.columbia.edu/">FlingBot</a> won <a href="https://sites.google.com/robot-learning.org/corl2021/program/awards_2021?authuser=0"> Best System Paper Award at CoRL</a> 2021!-->
<!--                    	<li> Honored to be a <a href="https://www.microsoft.com/en-us/research/academic-program/faculty-fellowship/#!fellows"> 2021 Microsoft Research Faculty Fellow </a> </li>-->
<!--                    	<li> TossingBot won <a href="https://www.ieee-ras.org/publications/t-ro"> 2020 IEEE Transactions on Robotics Best Paper Award! </a> </li>-->


<!--                        <span id="moreNews">-->
<!--                        <li><a href="https://form2fit.github.io/">  Form2Fit </a> got ICRA'20 <a href="https://form2fit.github.io/"> Best Paper in Automation </a> Award Finalist </li>-->
<!--                        <li><a href="https://illumination.cs.princeton.edu/"> Nerual Illumination </a> got CVPR'19 <a href="https://illumination.cs.princeton.edu/">Best Paper </a> Award Finalist</li>-->
<!--                        <li> <a href="https://www.jpmorgan.com/technology/artificial-intelligence/research-awards/faculty-research-awards-2021">  JP Morgan Faculty Research Award </a> 2021 </li>-->
<!--                        <li> <a href="https://www.tri.global/news/university-collab/"> TRI Young Faculty Award  </a> 2021</li>-->
<!--                        <li> <a href="https://www.amazon.science/research-awards/recipients"> Amazon Research Award  </a> 2020 </li>-->
<!--                        <li><a href="https://tossingbot.cs.princeton.edu/"> TossingBot </a> got RSS'19 <a href="https://tossingbot.cs.princeton.edu/">Best Systems Paper</a> Award and featured on the front page of <a href="images/nytimes-business-newspaper-tossingbot.png">The New York Times Business</a></li></li>-->
<!--                        <li><a href="https://vpg.cs.princeton.edu/">Learning Synergies between Pushing and Grasping</a>  got IROS <a href="http://vpg.cs.princeton.edu/">Best Cognitive Robotics Paper</a> Award Finalist</li>-->
<!--                        <li>Amazon Robotics <a href="http://arc.cs.princeton.edu">Best Systems Paper</a> Award</li>-->
<!--                        <li>1<sup>st</sup> place winners of the stow task at the worldwide <a href="https://www.amazonrobotics.com/">Amazon Picking Challenge</a> with <a href="https://mcube.mit.edu/">MIT</a></li>-->
<!--                        </span>-->
<!--                        <div onclick="toggleNews()" id="moreNewsBtn" class="showBtn"><a>Show more...</a></div>-->
<!--                        <div onclick="toggleNews()" id="lessNewsBtn" class="showBtn"><a>Show less...</a></div>-->
<!--                    </ul>-->
<!--                </p>-->
<!--                <div style="clear: both;"></div>-->
<!--            </div>-->
<!--            <div class="divider"></div>-->
<!--            <div class="section">-->
<!--                <h1>Recent Talks</h1>-->
<!--                <p>-->
<!--                    <ul>-->
<!--                        <li> Learning Meets Gravity: Robots that Embrace Dynamics and Pixels-->
<!--                        (<a href="https://www.youtube.com/watch?v=voiGM061dyw&ab_channel=NorthwesternRobotics"> Talk </a>)  at Northwestern Robotics </li>-->

<!--                        <li> Abstraction as Inductive Bias: Open-World 3D Scene Understanding without Open-World 3D Data-->
<!--                        (<a href="./talks/CoRL_inductive_bias.pdf"> Slide </a>)  at <a href="https://sites.google.com/view/corl-2022-inductive-bias-ws/home?authuser=0">  Inductive Bias in Robot Learning Workshop</a> @ CoRL'22</li>-->

<!--                        <li> The Reasonable Effectiveness of Dynamic Manipulation for Deformable Objects(<a href="https://www.youtube.com/watch?v=Pia-V67ishw&ab_channel=UniversityofTorontoRoboticsInstitute"> Talk </a>)  at University of Toronto Robotics Institute  </li>-->


<!--                        <li> Structure from Action: Articulated Object Structure Discovery with Active Interactions (<a href="./talks/StrcturefromAction.pdf"> Slides </a>)  at ICCV <a href="https://geometry.stanford.edu/struco3d/"> Struco3D Workshop </a>  </li>-->
<!--                        <li> Self-Adaptive Manipulation  (<a href="./talks/self-adaptive-grasping.pdf"> Slides </a>)  (<a href="https://www.youtube.com/watch?v=f2OZPwhSQu4&ab_channel=AllenInstituteforAI"> Talk </a>)  </li>-->
<!--                    	<li>Unfolding the Unseen: Deformable Cloth Perception and Manipulation  (<a href="./talks/unfold_unsceen.m4v"> Slides Video </a> <a href="./talks/Cloth_Precpetion.pdf"> PDF</a>)  (<a href="https://www.youtube.com/watch?v=Ek20Zw77QPU&ab_channel=3DGVSeminar"> Talk </a>)  </li>-->
<!--                        <li>Active Scene Understanding with Robot Interactions (<a href="./talks/2020_active_scene.pdf"> Slides </a>)  </li>-->

<!--                        <span id="moreTalks">-->
<!--                        <li>Category-level Pose Estimation (<a href="./talks/Pose_Estimation_ECCV_2020.pdf"> Slides </a>) at ECCV2020  <a href="http://cmp.felk.cvut.cz/sixd/workshop_2020/">  Workshop on Recovering 6D Object Pose </a></li>-->
<!--                        <li>Learning Visual Representations for Generalizable Manipulation  (<a href="./talks/visualrep_manuplation_cvpr2020.pdf"> Slides </a>) at CVPR2020 <a href="https://scene-understanding.com/">  3D Scene Understanding for Vision, Graphics, and Robotics </a></li>-->
<!--                        </span>-->

<!--                        <div onclick="toggleTalks()" id="moreTalksBtn" class="showBtn"><a>Show more...</a></div>-->
<!--                        <div onclick="toggleTalks()" id="lessTalksBtn" class="showBtn"><a>Show less...</a></div>-->

<!--                    </ul>-->
<!--                </p>-->
<!--                <div style="clear: both;"></div>-->
<!--            </div>-->

            <div class="divider"></div>
            <div class="section research">
                <h1>Publications</h1>
                <h3> * denotes equal contribution</h3>

<!--                <h3><div id="repBtn"><a href="javascript:showRep()">Representative</a></div>&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp;&nbsp;<div id="showAllBtn"><a href="javascript:hideRep()">See All Publications</a></div></h3>-->
                <div class="research-proj">
                    <a href="" class="research-thumb">
                        <div class="badge">CVPR 2025</div>
                        <img src="resources/images/projects/CVPR2025_capnet.gif" alt="" />
                    </a>
                    <a href="https://shanehuanghz.github.io/CAPNet" class="research-proj-title">  ðŸŽ©CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image </a>
                    <p> Jingshun Huang*, <strong>Haitao Lin*</strong>, Tianyu Wang, Yanwei Fu, Xiangyang Xue, Yi Zhu. <br>
                        CVPR, 2025 <highlight>(Hightlight, 3% of submission)</highlight> <br>
                       <a href="https://shanehuanghz.github.io/CAPNet">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Code </a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Dataset </a>&nbsp;
                    </p>
                    <p> <br> </p>
                </div>

                <div class="research-proj">
                    <a href="" class="research-thumb">
                        <div class="badge">ICRA 2025</div>
                        <img src="resources/images/projects/ICRA2025_yoeo.gif" alt="" />
                    </a>
                    <a href="https://shanehuanghz.github.io/YOEO/" class="research-proj-title">  You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping </a>
                    <p> Jingshun Huang*, <strong>Haitao Lin*</strong>, Tianyu Wang, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue. <br>
                       ICRA, 2025 <br>
                       <a href="https://shanehuanghz.github.io/YOEO/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Code </a>&nbsp;
                    </p>
                    <p> <br> </p>
                </div>

                <div class="research-proj">
                    <a href="https://star-uu-wang.github.io/Polaris/" class="research-thumb">
                        <div class="badge">IROS 2024</div>
                        <img src="resources/images/projects/IROS2024_polaris.gif" alt="" />
                    </a>
                    <a href="https://star-uu-wang.github.io/Polaris/" class="research-proj-title">  Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models </a>
                    <p> Tianyu Wang, <strong>Haitao Lin</strong>, Junqiu Yu, Yanwei Fu. <br>
                       IROS, 2024 <br>
                       <a href="https://star-uu-wang.github.io/Polaris/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2408.07975">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Code </a>&nbsp;
                    </p>
                    <p> <br> </p>
                </div>

                <div class="research-proj">
                    <a href="https://jrryzh.github.io/LAC-Net/" class="research-thumb">
                        <div class="badge">IROS 2024</div>
                        <img src="resources/images/projects/IROS2024_lacnet.gif" alt="" />
                    </a>
                    <a href="https://jrryzh.github.io/LAC-Net/" class="research-proj-title"> LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion  </a>
                    <p> Jinyu Zhang*, Yongchong Gu*, Jianxiong Gao, <strong>Haitao Lin</strong>, Qiang Sun, Xinwei Sun, Xiangyang Xue, Yanwei Fu. <br>
                       IROS, 2024 <br>
                       <a href="https://jrryzh.github.io/LAC-Net/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2308.15962">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Code </a>
                    </p>
                    <p> <br> </p>
                </div>


                <div class="research-proj">
                    <a href="https://hetolin.github.io/PourIt/" class="research-thumb">
                        <div class="badge">ICCV 2023</div>
                        <img src="resources/images/projects/ICCV2023_pourit.gif" alt="" />
                    </a>

                    <a href="https://hetolin.github.io/PourIt/" class="research-proj-title">  PourIt!: Weakly-supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring </a>
                    <p> <strong>Haitao Lin</strong>, Yanwei Fu, Xiangyang Xue. <br>
                       ICCV, 2023 <br>
                       <a href="https://hetolin.github.io/PourIt/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_PourIt_Weakly-Supervised_Liquid_Perception_from_a_Single_Image_for_Visual_ICCV_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/hetolin/PourIt">Code </a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://drive.google.com/drive/folders/1j3uIlCxFFMGply-gi-Wt26b-ivZFqpRr?usp=share_link">Dataset </a>

                    </p>
                    <p> <br> </p>
                </div>

                <div class="research-proj">
                    <a href="https://sunqiang85.github.io/FLarG/" class="research-thumb">
                        <div class="badge">IROS 2023</div>
                        <img src="resources/images/projects/IROS2023_flarg.jpg" alt="" />
                    </a>

                    <a href="https://sunqiang85.github.io/FLarG/" class="research-proj-title">  Language Guided Robotic Grasping with Fine-grained Instructions  </a>
                    <p>
                        Qiang Sun*, <strong>Haitao Lin*</strong>, Ying Fu, Yanwei Fu, Xiangyang Xue
                       <br>
                       IROS, 2023 <br>
                       <a href="https://sunqiang85.github.io/FLarG/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://sunqiang85.github.io/FLarG/">Code </a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf" class="research-thumb">
                        <div class="badge">CVPR 2023</div>
                        <img src="resources/images/projects/CVPR2023_geovln.png" alt="" />
                    </a>

                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf" class="research-proj-title">  GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation  </a>
                    <p>
                        Jingyang Huo*, Qiang Sun*, Boyan Jiang*, <strong>Haitao Lin</strong>, Yanwei Fu
                       <br>
                       CVPR, 2023 <br>
<!--                       <a href="https://jiashunwang.github.io/Neural-Pose-Transfer/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;-->
                       <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf">Paper</a>
<!--                       <a href="https://github.com/jiashunwang/Neural-Pose-Transfer/">Code </a>-->
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://hetolin.github.io/Skt_grasp/" class="research-thumb">
                        <div class="badge">ICRA 2023</div>
                        <img src="resources/images/projects/ICRA2023_eyegaze.gif" alt="" />
                    </a>

                    <a href="https://hetolin.github.io/Skt_grasp/" class="research-proj-title">  I Know What You Want: A "Smart Bartender" System by Interactive Gaze Following </a>
                    <p> <strong>Haitao Lin</strong>, Zhida Ge, Xiang Li, Yanwei Fu, and Xiangyang Xue  <br>
                       ICRA, Stand Alone Video, 2023 <br>
                       <a href="https://www.youtube.com/watch?v=zumlLDaXfMg">Demo</a>
<!--                       <a href="https://arxiv.org/pdf/2205.04026.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;-->
<!--                       <a href=" ">Code </a>-->
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://hetolin.github.io/SAR-Net/" class="research-thumb">
                        <div class="badge">CVPR 2022</div>
                        <img src="resources/images/projects/CVPR2022_sarnet.gif" alt="" />
                    </a>


                    <a href="https://hetolin.github.io/SAR-Net/" class="research-proj-title">  SAR-Net: Shape Alignment and Recovery Network for Category-level 6D Object Pose and Size Estimation </a>
                    <p> <strong>Haitao Lin</strong>, Zichang Liu, Chilam Cheang, Yanwei Fu, Guodong Guo, Xiangyang Xue <br>
                       CVPR, 2022 <br>
                       <a href="https://hetolin.github.io/SAR-Net/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SAR-Net_Shape_Alignment_and_Recovery_Network_for_Category-Level_6D_Object_CVPR_2022_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/hetolin/SAR-Net">Code </a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://hetolin.github.io/Skt_grasp/" class="research-thumb">
                        <div class="badge">ICRA 2022</div>
                        <img src="resources/images/projects/ICRA2022_sktgrasp.gif" alt="" />
                    </a>

                    <a href="https://hetolin.github.io/Skt_grasp/" class="research-proj-title">  I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches </a>
                    <p> <strong>Haitao Lin</strong>, Chilam Cheang, Yanwei Fu, Xiangyang Xue <br>
                       ICRA, 2022 <br>
                       <a href="https://hetolin.github.io/Skt_grasp/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2205.04026.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://baboon527.github.io/lang_6d/" class="research-thumb">
                        <div class="badge">ICRA 2022</div>
                        <img src="resources/images/projects/ICRA2022_langrasp.gif" alt="" />
                    </a>

                    <a href="https://baboon527.github.io/lang_6d/" class="research-proj-title">  Learning 6-DoF Object Poses to Grasp Category-level Objects by Language Instructions </a>
                    <p>
                       Chilam Cheang, <strong>Haitao Lin</strong>, Yanwei Fu, Xiangyang Xue
                       <br>
                       ICRA, 2022 <br>
                       <a href="https://baboon527.github.io/lang_6d/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/pdf/2205.04028.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a>
                    </p>
                </div>

                <div class="research-proj">
                    <a href="https://jiashunwang.github.io/Neural-Pose-Transfer/" class="research-thumb">
                        <div class="badge">CVPR 2020</div>
                        <img src="resources/images/projects/CVPR2020_neuralpose.jpg" alt="" />
                    </a>

                    <a href="https://jiashunwang.github.io/Neural-Pose-Transfer/" class="research-proj-title">  Neural Pose Transfer by Spatially Adaptive Instance Normalization  </a>
                    <p>
                        Jiashun Wang*, Chao Wen*, Yanwei Fu, <strong>Haitao Lin</strong>, Tianyun Zou, Xiangyang Xue, Yinda Zhang
                       <br>
                       CVPR, 2020 <br>
                       <a href="https://jiashunwang.github.io/Neural-Pose-Transfer/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Pose_Transfer_by_Spatially_Adaptive_Instance_Normalization_CVPR_2020_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/jiashunwang/Neural-Pose-Transfer/">Code </a>
                    </p>
                </div>


<!--                <div onclick="togglePubs()" id="morePubsBtn" class="showBtn"><a>Show more...</a></div>-->
<!--                <div onclick="togglePubs()" id="lessPubsBtn" class="showBtn"><a>Show less...</a></div>-->
                <div style="clear: both;"></div>

                <h1>Internship</h1>

<!--                <h3><div id="repBtn"><a href="javascript:showRep()">Representative</a></div>&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp;&nbsp;<div id="showAllBtn"><a href="javascript:hideRep()">See All Publications</a></div></h3>-->

                <div class="research-proj">
                    <a href="https://www.mech-mind.com.cn" class="research-thumb">
                       <img src="resources/images/intern/intern_mechmind.png" alt="" />
                    </a>

                    <p>
                    <a href="https://www.mech-mind.com.cn"  class="research-proj-title"> Deep Learning Algorithm and Software Group </a>
                    <br>
                        Computer Vision Algorithm Intern<br>
                    2023 <br>
                    </p>
                </div>




                <h1>Projects</h1>

<!--                <h3><div id="repBtn"><a href="javascript:showRep()">Representative</a></div>&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp;&nbsp;<div id="showAllBtn"><a href="javascript:hideRep()">See All Publications</a></div></h3>-->

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_nucleic_acid.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title"> Hybrid Visual and Tactile-Guided Nucleic Acid Throat Swab Sampling Robot</a>
                    <br>
                        The robotic arm is controlled to gently swab the throat with real-time visual and tactile feedback <br>
                    2022 <br>
                    </p>
                </div>

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_fetch_medicine.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title"> Robot Pharmacist</a>
                    <br>
                        The robotic arm is controlled to fetch bottles with the estimated 6-DoF pose<br>
                    2021 <br>
                    </p>
                </div>

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_legged_robot.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title">  Legged robot on Webots Simulation</a>
                    <br>
                        Trotting gait planning and attitude control of a legged robot<br>
                    2019 <br>
                    </p>
                </div>

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_fire_detection.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title"> Indoor Fire Detection by a Quadcopter</a>
                    <br>
                        Obtain the video stream from DJI Mavic Pro's camera and perform the fire detection algorithm on a mobile phone<br>
                    2018 <br>
                    </p>
                </div>

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_quadcopter.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title"> DIY Quadcopter</a>
                    <br>
                        A quadcopter is controlled using optical flow for position and an ultrasonic sensor for altitude<br>
                    2016 - 2017 <br>
                    </p>
                </div>

                <div class="research-proj">
                    <a class="research-thumb">
                        <video playsinline="" muted="" autoplay="" loop="" width="180px">
                        <source src="resources/images/projects/project_remote_quadcopter.mp4" type="video/mp4">
                        </video>
                    </a>

                    <p>
                    <a class="research-proj-title"> DIY Quadcopter</a>
                    <br>
                        My DIY quadcopter is flying<br>
                    2015 <br>
                    </p>
                </div>

            </div>

        </div>

        <script>
            function toggleNews() {
              var moreNews = document.getElementById("moreNews");
              var moreNewsBtn = document.getElementById("moreNewsBtn");
              var lessNewsBtn = document.getElementById("lessNewsBtn");
              if (moreNewsBtn.style.display === "none") {
                moreNews.style.display = "none";
                moreNewsBtn.style.display = "inline";
                lessNewsBtn.style.display = "none";
              } else {
                moreNews.style.display = "inline";
                moreNewsBtn.style.display = "none";
                lessNewsBtn.style.display = "inline";
              }
            }


            function toggleTalks() {
              var moreTalks = document.getElementById("moreTalks");
              var moreTalksBtn = document.getElementById("moreTalksBtn");
              var lessTalksBtn = document.getElementById("lessTalksBtn");

              if (moreTalksBtn.style.display === "none") {
                moreTalks.style.display = "none";
                moreTalksBtn.style.display = "inline";
                lessTalksBtn.style.display = "none";
              } else {
                moreTalks.style.display = "inline";
                moreTalksBtn.style.display = "none";
                lessTalksBtn.style.display = "inline";
              }
            }
            function togglePubs() {
              var morePubs = document.getElementById("morePubs");
              var morePubsBtn = document.getElementById("morePubsBtn");
              var lessPubsBtn = document.getElementById("lessPubsBtn");
              if (morePubsBtn.style.display === "none") {
                morePubs.style.display = "none";
                morePubsBtn.style.display = "inline";
                lessPubsBtn.style.display = "none";
              } else {
                morePubs.style.display = "inline";
                morePubsBtn.style.display = "none";
                lessPubsBtn.style.display = "inline";
              }
            }

            function showRep() {
              var repBtn = document.getElementById("repBtn");
              var showAllBtn = document.getElementById("showAllBtn");
              repBtn.style.color = "#49bf9d";
              repBtn.style.borderBottom = "1px solid #a4dfce";
              showAllBtn.style.color = "#191e3f";
              showAllBtn.style.borderBottom = "none";
              var  = document.getElementsByClassName("");
              for (var i = 0; i < .length; i++) {
                .item(i).style.display = "none";
              }
            }

            function hideRep() {
              var repBtn = document.getElementById("repBtn");
              var showAllBtn = document.getElementById("showAllBtn");
              repBtn.style.color = "#191e3f";
              repBtn.style.borderBottom = "none";
              showAllBtn.style.color = "#49bf9d";
              showAllBtn.style.borderBottom = "1px solid #a4dfce";
              var  = document.getElementsByClassName("");
              for (var i = 0; i < .length; i++) {
                .item(i).style.display = "table";
              }
            }
        </script>
    </body>
</html>