

<html>

<head>
    <link rel="StyleSheet" href="style.css" type="text/css" media="all">
    <title>I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches</title>
    <meta property="og:title" content="I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
    <br>
    <div class="center-div">
        <span style="font-size:40px">I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches</span>
    </div>

    <br>
    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:22px"><a href="#">Haitao Lin</a></span>
                    </div>
                </td>

                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:22px"><a href="#">Chilam Cheang</a></span>
                    </div>
                </td>
              
                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:22px"><a href="https://yanweifu.github.io/">Yanwei Fu</a></span>
                    </div>
                </td>

                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:22px"><a href="#">Xiangyang Xue</a></span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <br>
    <table align="center" width="700px">
        <tbody>
            <tr>
                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:22px">Fudan University</span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>
    
    <div class="center-div">
        <img class="round" style="height:200" src="./resources/images/teaser_comp.jpg">
    </div>
    <br><br>
    
    <hr>
    <div class="center-div">
        <h1>Abstract</h1>
    </div>
    <p style="text-align:justify">
        In this paper, we are interested in the problem of generating target grasps by understanding freehand sketches. The sketch is useful for the persons who cannot formulate language and the cases where a textual description is not available on the fly. However, very few works are aware of the usability of this novel interactive way between humans and robots. To this end, we propose a method to generate a potential grasp configuration relevant to the sketch-depicted objects. Due to the inherent ambiguity of sketches with abstract details, we take the advantage of the graph by incorporating the structure of the sketch to enhance the representation ability. This graph-represented sketch is further validated to improve the generalization of the network, capable of learning the sketch-queried grasp detection by using a small collection (around 100 samples) of hand-drawn sketches. Additionally, our model is trained and tested in an end-to-end manner which is easy to be implemented in real-world applications. Experiments on the multi-object VMRD and GraspNet-1Billion datasets demonstrate the good generalization of the proposed method. The physical robot experiments confirm the utility of our method in object-cluttered scenes.
    </p>
    <br><br>
    
    <hr>
    <div class="center-div">
        <h1 id="code">Network Architecture</h1>
    </div>
    <div class="center-div">
        <img class="round" style="height:300" src="./resources/images/architecture_revised_comp.jpg">
    </div>
    <br><br>
    

    <hr>
    <div class="center-div">
        <h1 id="paper">Paper</h1>
    </div>
    <table align="center" width="600px">

        <tbody>
            <tr>
                <td>
                    <img class="layered-paper-big" style="height:175px" src="resources/images/page1.png">
                </td>
                <td>
                    <span style="font-size:14pt">H. Lin, C. Cheang, Y. Fu, X. Xue</span>
                    <br><br>
                    <b><span style="display:inline-block;width:600px;font-size:14pt">I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches</span></b>
                    <br><br>
                    <span style="font-size:14pt">ICRA 2022.</span>
                    <br><br>
                    <span style="font-size:20px">
                        <a href="https://arxiv.org/abs/2205.04026">[arXiv]</a> &nbsp; &nbsp;
                    </span>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>

    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div">
                            <h1>Videos on bilibili</h1>
                        </div>
                        <iframe width="980" height="551.25" src="//player.bilibili.com/player.html?aid=684040294&bvid=BV1xU4y1m7vN&cid=720295145&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                    </left>
                </td>
            </tr>
        </tbody>
        
        <tbody>
            <tr>
                <td>
                    <left>
                        <iframe width="980" height="551.25" src="//player.bilibili.com/player.html?aid=214076116&bvid=BV1ka411J7d6&cid=720285675&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                    </left>
                </td>
            </tr>
        </tbody>
        
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div">
                            <h1>Videos on YouTube</h1>
                        </div>
                        <iframe width="980" height="551.25" src="https://www.youtube.com/embed/RIp4g7ibQ9o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </left>
                </td>
            </tr>
        </tbody>
        
        <tbody>
            <tr>
                <td>
                    <left>
                        <iframe width="980" height="551.25" src="https://www.youtube.com/embed/aEqF1yxwvqQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </left>
                </td>
            </tr>
        </tbody>

    </table>
    <br><br>
    
    <hr>
    <div class="center-div">
        <h1 id="code">Comparison Results on VMRD and GraspNet datasets</h1>
    </div>
    <div class="center-div">
        <img class="round" style="width:980px" src="./resources/images/compare_vis_comp.jpg">
    </div>
    <br><br>
    
    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div">
                            <h1>Acknowledgements</h1>
                        </div>
                        <div class="center-div">
                        Xiangyang Xue and Yanwei Fu are the corresponding authors. This work was supported in part by NSFC under Grant (No. 62076067), STCSM Project (19511120700), and Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103).
                            The website is modified from this <a href="https://walsvid.github.io/Pixel2MeshPlusPlus/">template</a>.
                        </div>
                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>



</body>

</html>
